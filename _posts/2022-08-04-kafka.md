---
layout: post
title:  "kafka 기초 "
date:   2022-08-04 16:20:21 +0900
categories: kafka
---

# 카프카 기초

## Kafka feature

|kafka producer| kafka| kafka consumer|
|Source App| topic1, topic2 .. | TargetApplication|

## Kafka topic
- 데이터가 들어갈 수 있는 공간.
- 여러개의 토픽을 생산할 수 있음.
- 이름을 가질 수 있음
- 토픽별로 여러개의 파티션이 있다.
    - 0번부터 시작한다.
- consumer가 데이터를 가져가지만, 데이터는 삭제되지 않는다.
- 파티션의 삭제가 되는 시점은 최대 record 보존시간
- 키를 지정하지 않으면 라운드로빈으로 파티션이 지정됨
- 키를 지정하면 특정 파티션에 할당
- `파티션을 늘릴 수 있지만, 줄일 수 없다.(유의)`

## broker
- 카프카가 설치되어있는 서버단위
    - 보통 3개 이상 구성

## replication
- 파티션 복제를 뜻함
- replication 1 -> partition 1
- replication 2 -> 원본 1개 복제본 1개

## 파티셔너
- 프로듀서가 데이터보내면, 무조건 파티셔너를 통해 브로커로 데이터가 전송됨
- 어떤 파티션에 전송할지 설정
- message-key
    - 파티셔너에 의해 해시값이 결정됨

## lag
- 프로듀셔가 데이터를 넣는것이 컨슈머에서 데이터를 가져가는것보다 속도가 빠르다면 오프셋간의 차이가 발생
- 주로 컨슈머 상태에대해 볼때 확인
- 프로듀서가 넣은 데이터의 오프셋과 컨슈머가 가져가는 데이터의 오프셋 차이를 기반으로 함
- 렉은 여러개 존재할 수 있다(-> 파티션이 여러개 있기 때문에)

## 버로우
- 멀티 카프카 클러스터 lag을 모니터링 가능
- 슬라이딩 윈도으를 통해 컨슈머 스테이터스를 확인가능

## rabbitMQ, resdis Queue, kafka 차이점
- 메시지 브로커는 앞의 둘
- 이벤트 브로커는 카프카

## 프로듀서
- 데이터를 생산하는 역할(카프카에 보내는 역할)
- kafka topic에 셍신 
- 데이터 적재
- role
    - 토픽에 해당하는 메시지를 생성
    - 특정 토픽으로 데이터를 publish
    - 처리 실패/재시도

```java

public class Producer {
    main() {
        Properties configs = new Properties();
        configs.put("bootstrap.servers", "localhost:9092"); 
        configs.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        configs.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(configs);
        // 키없이 보냄.
        ProducerRecord record = new ProducerRecord<String, String> ("click_log", "login");
        // 키를 같이 보내고싶다면 
        ProducerRecord record = new ProducerRecord<String, String> ("click_log", 1, "login");
        producer.send(record);
        producer.close();
    }
}

```

## 컨슈머
- 데이터를 가져가도 사라지지 않는다.
- 데이터 파이프라인
- 토픽의 데이터를 가져온다.
- 폴링
- role
    - Topic의 파티션으로부터 데이터 폴링
    - 파티션 오프셋의 위치 기록 (commit)
    - 컨슈머 그룹을 통해 병렬처리

```java
public class Consumer {
    main() {
        Properties configs = new Properties();
        configs.put("bootstrap.servers", "localhost:9092"); 
        configs.put("group.id", "click_log_group"); 
        configs.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        configs.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(configs);
        consumer.subscribe(Arrays.asList("click_log"));

        while(true) {
            ConsumerRecord<String, String> records = consumer.poll(500); // 0.5초동안 데이터가 도착하기를 기다림, 들어오지 않으면 빈 레코드 반환
            for (ConsumerRecord<String, String> record : records) {
                sout(record.value());
            }
        }

    }
}
```

## apache kafka stream
- 카프카에서 공식적으로 제공하는 라이브러리
- 스프링부트에 올려서 사용가능하다
- 장점
    - 카프카와 완벽호환
    - 스케쥴링 도구가 필요없다.
    - 스파크 스트리밍을 사용하면 이벤트 데이터 애플리케이션을 만든다.
        - 클러스터 관리자나 매니저를 구축해야한다.
    - 스트림즈DSL 사용(이벤트기반 다양한 기능들을 제공), 
    - 로컬 상태저장소를 사용한다.
        - 실시간 방식
            - 비상태 기반처리
                - 데이터들어오는 족족 바로 프로듀스 (유실 중복 없음)
            - 상태기반 처리 
            