---
layout: post
title:  "아파치 카프카 스터디 - 3"
date:   2023-01-16 12:20:21 +0900
categories: kafka, spring
---

> 아프치 카프카 애플리케이션 프로그래밍 스터디 - 3


# 목표?
- 회사에서 대용량 데이터를 처리하는것에 있어, 카프카를 사용하고 있으므로, 제대로된 학습을 통해 원하는 처리를 할 수 있도록 한다.


# 컨슈머 API
- 프로듀서가 전송한 데이터는 카프카 브로커에 적재가 된다. 컨슈머는 적재된 데이터를 사용하기 위해 브로커로부터 데이터를 가져와 필요한 처리를 한다.

## 중요개념
- 토픽의 파티션으로부터 데이터를 가져가기 위해 컨슈머를 운영하는 방법은 크게 2가지가 있다. 
    1. 1개 이상의 컨슈머 그룹을 운영
        - 컨슈머를 각 컨슈머 그룹으로부터 격리된 환경에서 안전하게 운영할 수 있도록 도와주는 카프카의 방식
        - 컨슈머 그룹으로부터 묶인 컨슈머들은 토픽의 1개 이상 파티션들에 할당되어 데이터를 가져갈 수 있다.
        - 컨슈머 그룹으로 묶인 컨슈머가 토픽을 구독해서 데이터를 가져갈 때, 1개의 파티션은 최대 1개의 컨슈머 컨슈머에 할당 가능하며, 1개 컨슈머는 여러개의 파티션에 할당될 수 있다.
        - 컨슈머 그룹의 컨슈머 개수는 가져가고자 하는 토픽의 파티션 개수보다 같거나 작어야한다.
        - 파티션이 3개인대, 컨슈머가 4개로 이루어진다면 하나는 불필요한 스레드로 남아 `유휴` 상태로 남겨지게 된다.
    2. 토픽의 특정 파티션만 구독하는 컨슈머를 운영
- enable.auto.commit = true
    - poll() 메서드가 수행될 때 일정 간격 마다 자동 커밋되는것을 오프셋 커밋
    - 컨슈머의 자동 종료 발생 또는 `리벨런싱` 시 컨슈머가 처리하는 데이터가 중복 또는 유실될 수 있는 취약한 구조를 가지고 있다. 데이터의 중복 또는 유실을 허용하지 않는 서비스라면 명시적으로 오프셋 커밋을 해야한다.(commiySync() poll 호출 후 반환된 레코드의 가장 마지막 오프셋을 기준으로 커밋을 수행)
- 옵션
    - group.id: 컨슈머 그룹 아이디를 지정한다. subscribe() 메서드로 토픽을 구독하여 사용할 때는 이 옵션을 필수로 넣어야한다. 기본값은 Null
    - auto.offset.reset: 컨슈머 그룹이 특정 파티션을 읽을 때 지정된 컨슈머 오프셋이 없는 경우 어느 오프셋부터 읽을지 선택하는 옵션
        - latest: 가장 최근에 넣은것부터
        - earliest: 가장 나중에 넣은것부터
        - none: 커밋된 기록이 있는지 확인해본 후, 있으면 기존 커밋기록부터 읽고 없으면 오류를 반환
    - enable.auto.commit: 자동 커밋으로 할지 수동 커밋으로 할지 결정
    - auto.commit.interval.ms: 자동 커밋일 경우 오프셋 커밋 간격을 지정한다.
    - max.poll.records: poll() 메서드를 통해 반환되는 레코드 개수를 지정한다. 기본값은 500
    - session.timeout.ms: 컨슈머가 브로커와 연결이 끊기는 최대시간이다. 이 시간에 하트비트를 전송하지 않으면 브로커는 컨슈머에 이슈가 발생했다고 가정하고 리벨런싱한다. 보통 하트비트 시간 간격의 3배로 설정한다. 기본 값은 10초
    - hearbeat.interval.ms: poll() 하트비트를 전송하는 시간 간격. 기본값은 3000ms
    - max.poll.interval.ms: poll() 메서드를 호출하는 간격의 최대 시간을 지정한다. poll() 메서드를 호출한 이후에 데이터를 처리하는 데에 시간이 너무 많이 걸리는 경우 비정상으로 판단하고 리밸런싱을 호출. 기본값은 5분
    - isolation.level: 트랜잭션 프로듀서가 레코드를 트랜잭션 단위로 보낼 경우 사용. 이 경우 read_committed, read_uncommiited로 설정할 수 있다. read_commited로 설정하면 커밋이 완료된 레코드만 읽는다. read_uncommitted는 커밋여부와 상관없이 모든 레코드를 읽는다. 기본 값은 read_uncommitted

## Admin API
- 실제 운영환경에서 프로듀서와 컨슈머를 통해 데이터를 주고받는 것만큼 카프카에 설정된 내부 옵션을 설정하고 확인하는것이 중요하다.
- 내부 옵션을 확인하는 것은 직접 브로커에 접속하여 카프카 브로커 옵션을 확인하는 것이지만 매우 번거로움
- 카프카 클라이언트에서는 내부 옵션을 설정하거나 조회하기 위해 AdminClient 클래스를 제공한다.
- AdminClient
    - 카프카 컨슈머를 멀티 스레드로 생성할 때 구독하는 토픽의 파티션 개수만큼이나 스레드를 생성하고 싶을 때, 스레드 생성전에 해당 토픽의 파티션 개수를 어드민 API를 통해 가져올 수 있다.
    - AdminClient 클래스로 구현한 웹 대시보드를 통해 ACL(Acess Control List)이 적용된 클러스터의 리소스 접근 권한 규칙 추가를 할 수 있다.
    - 특정 토픽의 트래픽이 늘어남을 감지하고, AdminClient 클래스로 해당 토픽의 파티션을 늘릴 수 있다.
```java
public class AdminExample {

    private final static Logger logger = LoggerFactory.getLogger(AdminExample.class);


    public static void main(String[] args) throws ExecutionException, InterruptedException {
        Properties configs = new Properties();
        configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "my-kafka:9092");

        AdminClient adminClient = AdminClient.create(configs);

        // 브로커별 설정
        for (Node node : adminClient.describeCluster().nodes().get()) {
            logger.info("node {}", node);
            ConfigResource cr = new ConfigResource(ConfigResource.Type.BROKER, node.idString());
            DescribeConfigsResult describeConfigs = adminClient.describeConfigs(Collections.singleton(cr));
            describeConfigs.all().get()
                    .forEach((broker, config) -> {
                        config.entries().forEach(configEntry -> logger.info(configEntry.name() + "= " + configEntry.value()));
                    });
        }

        // 토픽정보
        Map<String, TopicDescription> test = adminClient.describeTopics(Collections.singleton("test")).all().get();
        logger.info("map {}", test);
        adminClient.close(); // 명시적으로 종료 메서드를 호출하여 리소스 낭비를 하지 않게 만든다.
    }
}

```